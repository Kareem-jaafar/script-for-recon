
#!/bin/bash

# --- Configuration ---
# Set the name of the directory where all results will be stored
OUTPUT_DIR="Recon_$(date +%F_%H-%M-%S)"

# Wordlist for directory brute-forcing (must exist on your system)
# CHANGE THIS PATH to a valid wordlist, e.g., /usr/share/wordlists/dirb/common.txt
WORDLIST="/usr/share/wordlists/dirbuster/directory-list-2.3-small.txt"

# List of User-Agents to rotate through for better stealth
UAGENTS=(
"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36"
"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Safari/605.1.15"
"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0"
)

# --- Utility Functions ---

# Picks a random User-Agent string
pick_ua() {
Â  Â  echo "${UAGENTS[$RANDOM % ${#UAGENTS[@]}]}"
}

# Function to check if a necessary tool is installed
check_tool() {
Â  Â  command -v "$1" >/dev/null 2>&1 || { echo -e "\n[ERROR] Required tool '$1' is missing. Please install it to continue." ; exit 1; }
}

# --- Tool Checks (Required for your script) ---
# Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
check_tool "nmap"
check_tool "subfinder"
check_tool "assetfinder"
check_tool "amass"
check_tool "httprobe"
check_tool "ffuf"
# Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© ÙˆØ§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
check_tool "waybackurls" # Ù„Ø¬Ù…Ø¹ Ø¹Ù†Ø§ÙˆÙŠÙ† URL Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ©
check_tool "whatweb" Â  Â  # Ù„Ø¨ØµÙ…Ø§Øª Ø§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§

# --- Main Scan Function ---
scan_domain() {
Â  Â  local domain=$1
Â  Â  TARGET="$OUTPUT_DIR/$domain"
Â  Â  mkdir -p "$TARGET"
Â  Â  REPORT_SUMMARY="$TARGET/report_summary.txt"

Â  Â  echo -e "\n=======================================================" >> "$REPORT_SUMMARY"
Â  Â  echo -e "ðŸš€ Starting Advanced Recon for: $domain" >> "$REPORT_SUMMARY"
Â  Â  echo -e "=======================================================\n" >> "$REPORT_SUMMARY"
Â  Â  echo -e "\n[INFO] Starting Advanced Reconnaissance for $domain (PID: $$)..."

Â  Â  # --- 1. Subdomain Enumeration (Multi-tool approach) ---
Â  Â  echo -e "\n[INFO] Running Subdomain Enumeration (Subfinder, Assetfinder, Amass)..."
Â  Â  sleep $((RANDOM % 4 + 1))
Â  Â  subfinder -silent -all -d "$domain" -t 10 > "$TARGET/subs_raw.txt"
Â  Â  sleep $((RANDOM % 3 + 1))
Â  Â  assetfinder --subs-only "$domain" >> "$TARGET/subs_raw.txt"
Â  Â  sleep $((RANDOM % 3 + 1))
Â  Â  amass enum -passive -d "$domain" -o "$TARGET/amass_temp.txt" >> "$TARGET/subs_raw.txt"
Â  Â  
Â  Â  # Sort and unique the final list
Â  Â  sort -u "$TARGET/subs_raw.txt" -o "$TARGET/01_subdomains.txt"
Â  Â  rm -f "$TARGET/amass_temp.txt" "$TARGET/subs_raw.txt" # Clean up temp files

Â  Â  # --- 2. Check for Live Subdomains (httprobe) ---
Â  Â  echo -e "\n[INFO] Checking for live HTTP/HTTPS services (httprobe)..."
Â  Â  cat "$TARGET/01_subdomains.txt" | httprobe -c 30 -t 5000 > "$TARGET/02_alive_subdomains.txt"

Â  Â  echo "--- Live Subdomains Found ---" >> "$REPORT_SUMMARY"
Â  Â  cat "$TARGET/02_alive_subdomains.txt" >> "$REPORT_SUMMARY"
Â  Â  echo -e "\nLive subdomains saved to: $TARGET/02_alive_subdomains.txt"

Â  Â  # --- 3. Port Scanning (Stealthy Full Scan) ---
Â  Â  echo -e "\n[INFO] Running Nmap (Stealthy Full Port Scan)..."
Â  Â  # -T2 is slower but stealthier
Â  Â  nmap -sS -Pn -T2 -p- "$domain" --scan-delay 100ms --max-rate 50 -oN "$TARGET/03_ports.txt"
Â  Â  
Â  Â  echo -e "\n--- Open Ports ---" >> "$REPORT_SUMMARY"
Â  Â  # Extract "open" ports and append to the summary
Â  Â  grep "open" "$TARGET/03_ports.txt" | grep -v "filtered" >> "$REPORT_SUMMARY"
Â  Â  echo -e "\nNmap results saved to: $TARGET/03_ports.txt"

Â  Â  # --- 4. Directory Brute-forcing (ffuf) on Live Subdomains ---
Â  Â  echo -e "\n[INFO] Running FFUF for common file paths on live subdomains..."
Â  Â  
Â  Â  if [ ! -f "$WORDLIST" ]; then
Â  Â  Â  Â  echo -e "\n[WARNING] Wordlist not found at: $WORDLIST. Skipping FFUF."
Â  Â  Â  Â  echo "--- Directory Scan Skipped (Wordlist Missing) ---" >> "$REPORT_SUMMARY"
Â  Â  else
Â  Â  Â  Â  echo -e "\n--- File Paths / Directories Found (Status 200, 3xx) ---" >> "$REPORT_SUMMARY"
Â  Â  Â  Â  
Â  Â  Â  Â  cat "$TARGET/02_alive_subdomains.txt" | while read url; do
Â  Â  Â  Â  Â  Â  ua=$(pick_ua)
Â  Â  Â  Â  Â  Â  host=$(echo "$url" | sed 's~http[s]*://~~g')
Â  Â  Â  Â  Â  Â  out="$TARGET/ffuf_$(echo "$host" | tr '/' '_').csv"
Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  # ffuf command with stealth and User-Agent rotation
Â  Â  Â  Â  Â  Â  ffuf -w "$WORDLIST" -u "$url/FUZZ" -H "User-Agent: $ua" -mc 200,204,301,302,307 -rate 40 -timeout 5 -of csv -o "$out" >/dev/null 2>&1
Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  # Extract readable output for the summary
Â  Â  Â  Â  Â  Â  if [ -f "$out" ]; then
Â  Â  Â  Â  Â  Â  Â  Â  echo "Domain: $host" >> "$REPORT_SUMMARY"
Â  Â  Â  Â  Â  Â  Â  Â  awk -F',' '{print $4 " (" $3 ") - Size: " $5 " bytes"}' "$out" | grep -v 'url (status' >> "$REPORT_SUMMARY"
Â  Â  Â  Â  Â  Â  fi
Â  Â  Â  Â  Â  Â  sleep $((RANDOM % 2 + 1)) # Random delay between targets
Â  Â  Â  Â  done
Â  Â  fi

Â  Â  # --- 5. Wayback Machine URLs (Initial Fetch) ---
Â  Â  echo -e "\n[INFO] Collecting URLs from Wayback Machine (curl)..."
Â  Â  ua=$(pick_ua)
Â  Â  curl -A "$ua" --compressed -m 10 -s "http://web.archive.org/cdx/search/coll?url=*.$domain/*&output=txt&fl=original" > "$TARGET/wayback_temp.txt"

Â  Â  # --- 6. Advanced URL Discovery and Parameter Extraction (waybackurls) ---
Â  Â  echo -e "\n[INFO] Running Advanced URL Discovery (waybackurls)..."
Â  Â  cat "$TARGET/wayback_temp.txt" | waybackurls >> "$TARGET/04_urls_raw.txt"
Â  Â  cat "$TARGET/wayback_temp.txt" >> "$TARGET/04_urls_raw.txt" # Include curl results
Â  Â  rm -f "$TARGET/wayback_temp.txt"

Â  Â  # Sort unique and extract final URLs
Â  Â  sort -u "$TARGET/04_urls_raw.txt" -o "$TARGET/04_all_endpoints.txt"
Â  Â  
Â  Â  # Extract only URLs that contain parameters (potential injection points)
Â  Â  grep '=' "$TARGET/04_all_endpoints.txt" | sort -u > "$TARGET/05_endpoints_with_params.txt"

Â  Â  echo -e "\n--- Total Endpoints Found ---" >> "$REPORT_SUMMARY"
Â  Â  wc -l "$TARGET/04_all_endpoints.txt" >> "$REPORT_SUMMARY"
Â  Â  echo -e "\n--- Endpoints with Parameters ---" >> "$REPORT_SUMMARY"
Â  Â  wc -l "$TARGET/05_endpoints_with_params.txt" >> "$REPORT_SUMMARY"
Â  Â  
Â  Â  echo -e "All endpoints saved to: $TARGET/04_all_endpoints.txt"
Â  Â  echo -e "Parameter-rich endpoints saved to: $TARGET/05_endpoints_with_params.txt"

Â  Â  # --- 7. Technology Fingerprinting (whatweb) ---
Â  Â  echo -e "\n[INFO] Running Technology Fingerprinting (whatweb)..."
Â  Â  WHATWEB_FILE="$TARGET/06_technology_fingerprint.txt"
Â  Â  echo "--- Technology Fingerprinting (WhatWeb) ---" >> "$REPORT_SUMMARY"
Â  Â  
Â  Â  # Fingerprint the main domain
Â  Â  echo "Scanning Main Domain: $domain" >> "$WHATWEB_FILE"
Â  Â  whatweb "$domain" >> "$WHATWEB_FILE"
Â  Â  
Â  Â  # Fingerprint the live subdomains
Â  Â  cat "$TARGET/02_alive_subdomains.txt" | while read url; do
Â  Â  Â  Â  echo -e "\nScanning Subdomain: $url" >> "$WHATWEB_FILE"
Â  Â  Â  Â  whatweb -a 1 "$url" >> "$WHATWEB_FILE"
Â  Â  done
Â  Â  
Â  Â  echo "Technology details saved to: $WHATWEB_FILE"

Â  Â  echo -e "\n[SUCCESS] Scan for $domain complete. Detailed summary in $REPORT_SUMMARY"
}

# --- Main Execution ---

# 1. Check for required arguments
if [ "$#" -eq 0 ]; then
Â  Â  echo "Usage: $0 <domain1> [domain2] [domain3] ..."
Â  Â  echo "Example: $0 example.com test.org"
Â  Â  exit 1
fi

# 2. Create the main output directory
mkdir -p "$OUTPUT_DIR"
echo -e "Starting advanced reconnaissance scan for $# domain(s)..."
echo -e "Results will be stored in the directory: $OUTPUT_DIR"

# 3. Loop through all provided domains (Parallel Processing)
for d in "$@"; do
Â  Â  scan_domain "$d" & # Run in background
Â  Â  sleep 2 # Small delay between starting scans to avoid flooding the system/network
done

# 4. Wait for all background jobs to finish
wait

echo -e "\n\n======================================================="
echo "âœ… All advanced scans complete!"
echo "Find detailed results and summaries in the '$OUTPUT_DIR' directory."
echo "======================================================="
